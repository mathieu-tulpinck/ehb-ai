{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- https://lifewithdata.com/2022/03/13/how-to-remove-highly-correlated-features-from-a-dataset/\n",
    "- https://app.pluralsight.com/player?course=building-regression-models-scikit-learn&author=janani-ravi&name=1616b48f-65fd-4abd-b9fa-7a2560c9d5de&clip=3\n",
    "- https://towardsdatascience.com/interpreting-coefficients-in-linear-and-logistic-regression-6ddf1295f6f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "\n",
    "|General Notation | Description | Python (if applicable) |\n",
    "|---|---|---|\n",
    "| $a$ | scalar ||\n",
    "| $\\vec{a}$ | vector ||\n",
    "| $A$ | matrix ||\n",
    "| **Multiple variable regression** | | |\n",
    "| $x$ | \"input\" variable, feature ||\n",
    "| $y$ | \"output\" variable, target ||\n",
    "|  $X$ | training example matrix | `X_train` |   \n",
    "|  $\\vec{y}$  | training example  targets | `y_train` |\n",
    "|  $\\vec{x}^{(i)}$| features of $ith$ Training example | |\n",
    "|  $\\vec{x}^{(i)}$, $y^{(i)}$ | $i{th}$ Training example | |\n",
    "| $x_n^{(i)}$ | value of feature n in ith training example ||\n",
    "| m | number of training examples | `m` |\n",
    "| n | number of features in each example | `n` |\n",
    "|  $\\vec{w}$  |  parameter: weights | `w` |\n",
    "| $b$ | parameter: y-intercept | `b` |     \n",
    "| $f_{\\vec{w},b}(\\vec{x}^{(i)})$ | the result of the model evaluation at $\\vec{x}^{(i)}$ parameterized by $\\vec{w},b$: $f_{\\vec{w},b}(\\vec{x}^{(i)}) = \\vec{w} \\cdot \\vec{x}^{(i)}+b$  | `f_wb` | \n",
    "| **Gradient descent** | | |\n",
    "| $\\alpha$ | learning rate ||\n",
    "| $\\frac{\\partial J(\\vec{w},b)}{\\partial w_n}$ | partial derivative term ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and data import\n",
    "\n",
    "You are asked to predict a final grade of the math course based on the information we have about the student. The dataset is provided in the accompanying file 'student-mat.csv'. A full description of the data set can be found in the file 'metadata.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read from csv.\n",
    "mathscores = pd.read_csv('./data/student-mat.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathscores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Multiple variable linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn is a library that allows a.o. to generate heatmaps. Before calling the library's methods, the correlation matrix of the dataset is computed. In order to perform dimensionality reduction, we want to exclude the variables from the dataset which represent the same information as the one we try to predict. \n",
    "\n",
    "The heatmap hereunder shows a very high correlation between G1, G2 and G3 (respectively 0.8 and 0.9). For the purpose at hand, this means that adding G1 and G2 to the dataset would likely increase the predictive power of our model. However, if we are interested in understanding the socio-economic features having a high impact on students' results, we are better off discarding them.\n",
    "\n",
    "Another approach would be to do feature engineering and compute an average of G1, G2 and G3 and set that as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corr_matrix = mathscores.corr()\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "sns.heatmap(corr_matrix, annot = True, cmap=\"Blues\")\n",
    "\n",
    "# Alternative to heatmap\n",
    "# print(mathscores.corr()['G3'].sort_values())\n",
    "mathscores_without_G1_G2_G3 = mathscores.drop(['G1', 'G2', 'G3'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to pre-process the dataset before feeding it to the machine learning model. Pre-processing means performing scaling of numeric features and/or encoding of categorical features in order to regularize the data. This increases the efficiency of the machine learning step.\n",
    "\n",
    "We will encode categorical data using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mathscores_without_G1_G2_G3.info()\n",
    "\n",
    "# Encoding of categorical features\n",
    "categorical_features = mathscores_without_G1_G2_G3.select_dtypes(exclude = ['int64'])\n",
    "numeric_features = mathscores_without_G1_G2_G3.select_dtypes(include = ['int64'])\n",
    "categorical_features_cols = categorical_features.columns.values.tolist()\n",
    "numeric_features_cols = numeric_features.columns.values.tolist()\n",
    "concatenated_cols = categorical_features_cols + numeric_features_cols\n",
    "\n",
    "enc = OrdinalEncoder(dtype = 'int64')\n",
    "categorical_features = enc.fit_transform(categorical_features)\n",
    "\n",
    "concatenated = np.concatenate((categorical_features, numeric_features), axis = 1)\n",
    "X = pd.DataFrame(concatenated, columns = concatenated_cols)\n",
    "\n",
    "print(X.info())\n",
    "\n",
    "#X.to_csv(r'./data/concatenated.csv', index = None, header=True)\n",
    "\n",
    "y = mathscores['G3']\n",
    "\n",
    "# 80% - 20% split for the training and testing sets. 316/395 = 0.8 \n",
    "# Assign train and test sets (in your experiments, you want to do cross-validation).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(f\"X shape: {X_train.shape}, X type:{type(X_train)})\")\n",
    "print(f\"y shape: {y_train.shape}, y type:{type(y_train)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe target\n",
    "print(mathscores['G3'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit the regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical terms, the model function can be expressed as:\n",
    "\n",
    "$f_{\\vec{w},b}(\\vec{x}^{(i)}) = \\vec{w} . \\vec{x} + b$\n",
    "\n",
    "The values of the $\\vec{w}$ vector are called the weights. $b$ is a scalar value and is called the y-intercept. The goal is to find values for these parameters so that $J(\\vec{w}, b)$ - the cost function applied to arguments $\\vec{w}$, $b$ - is close to zero, meaning that the values cause the algorithm to fit the training set very well. Gradient descent is an algorithm that aims to achieve this task as efficiently as possible by repeatedly taking steps in the direction of steepest decrease of $J$.\n",
    "\n",
    "The algorithm can be formalized as follows:\n",
    "\n",
    "\n",
    "\n",
    "$$\\text{repeat} \\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w_n = w_n -  \\alpha \\frac{\\partial J(\\vec{w},b)}{\\partial w_n}  \\; \\newline\n",
    "\n",
    "b = b -  \\alpha \\frac{\\partial J(\\vec{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\frac{\\partial J(\\vec{w},b)}{\\partial w_n}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})x_n^{(i)} \\newline\n",
    "  \\frac{\\partial J(\\vec{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = linear_model.coef_\n",
    "df_w = pd.DataFrame(w, X.columns, columns=['coef']).sort_values(by='coef', ascending=False)\n",
    "\n",
    "print(df_w)\n",
    "\n",
    "b = linear_model.intercept_\n",
    "\n",
    "print(f\"b = {b:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction on training set:\\n{linear_model.predict(X_train)[:10]}\")\n",
    "# @ sign computes the dot product of vectors X[i] and w.\n",
    "print(f\"prediction using w,b:\\n{(X_train @ w + b)[:10]}\")\n",
    "print(f\"Target values:\\n{y_train[:10]}\")\n",
    "\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "df_pred_actual = pd.DataFrame({'predicted': y_pred, 'actual': y_test})\n",
    "\n",
    "df_pred_actual.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predicted vs actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.plot(y_test.values, label='Actual')\n",
    "\n",
    "plt.ylabel('G3')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a166168c68f55598b01c42966d36055a72b05dd81029221af08ea37c30386052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
